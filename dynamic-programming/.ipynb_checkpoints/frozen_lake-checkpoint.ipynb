{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# Dynamic Programming, Frozen Lake\n",
    "\n",
    "</center>\n",
    "\n",
    "In the FrozenLake environment, the agent navigates a 4x4 gridworld\n",
    "\n",
    "<img src=\"../_aux/images/frozen-lake-6.jpg\">\n",
    "Source: http://eskipaper.com/images/frozen-lake-6.jpg\n",
    "\n",
    "**Env description:**\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "\n",
    "If you step into one of those holes, you'll fall into the freezing water.\n",
    "At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "you navigate across the lake and retrieve the disc.\n",
    "\n",
    "However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "The surface is described using a grid like the following \n",
    "    \n",
    "S<font color='blue'>FFF</font><br>\n",
    "<font color='blue'>F</font><font color='red'>H</font><font color='blue'>F</font><font color='red'>H</font><br>\n",
    "<font color='blue'>FFF</font><font color='red'>H</font><br>\n",
    "<font color='red'>H</font><font color='blue'>FF</font>G<br>\n",
    "\n",
    "S : starting point, safe<br>\n",
    "F : frozen surface, safe<br>\n",
    "H : hole, fall to your doom<br>\n",
    "G : goal, where the frisbee is located<br>\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole.\n",
    "You receive a reward of 1 if you reach the goal, and zero otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,copy\n",
    "import numpy as np\n",
    "\n",
    "import check_test\n",
    "from frozenlake import FrozenLakeEnv\n",
    "from plot_utils import plot_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv(is_slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment Note:**\n",
    "\n",
    "The agent moves through a **$4 \\times 4$ gridworld**, with states numbered as follows:\n",
    "```\n",
    "[[ 0  1  2  3]\n",
    " [ 4  5  6  7]\n",
    " [ 8  9 10 11]\n",
    " [12 13 14 15]]\n",
    "```\n",
    "and the agent has **4 potential actions**:\n",
    "```\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state space: Discrete(16)\n",
      "Action_space: Discrete(4)\n",
      "total number of states: 16\n",
      "total number of actions: 4\n"
     ]
    }
   ],
   "source": [
    "# print the state space and action space, total number of states and actions\n",
    "print(f'state space: {env.observation_space}')\n",
    "print(f'Action_space: {env.action_space}')\n",
    "print(f'total number of states: {env.nS}')\n",
    "print(f'total number of actions: {env.nA}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333, 1, 0.0, False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at one-step dynamics of the Markov decision process (MDP)\n",
    "prob, next_state, reward, done = env.P[1][0][0] \n",
    "prob, next_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Policy\n",
    "\n",
    "<img src=\"../_aux/images/policy-eval.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `policy`: This is a 2D numpy array with `policy.shape[0]` equal to the number of states (`env.nS`), and `policy.shape[1]` equal to the number of actions (`env.nA`).  `policy[s][a]` returns the probability that the agent takes action `a` while in state `s` under the policy.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "- `theta`: This is a very small positive number that is used to decide if the estimate has sufficiently converged to the true value function (default value: `1e-8`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):                                                  # loop for every state\n",
    "            Vs = 0\n",
    "            for a, action_prob in enumerate(policy[s]):                          # loop for every action\n",
    "                for prob, next_state, reward, done in env.P[s][a]:               # loop for every probability of each possible reward and next state \n",
    "                    Vs += action_prob * prob * (reward + gamma * V[next_state])  # calculate Vscore\n",
    "            delta = max(delta, np.abs(V[s]-Vs))                                  # calculate delta\n",
    "            V[s] = Vs                                                            # update state-value ?? based on what ??\n",
    "        if delta < theta:                                                        # check process, if all state has been optimized (small delta, than)\n",
    "            break\n",
    "    return V                                                                     # look for that video that show the gradual change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note:**<br>\n",
    "compared the simple 2x2 grid world with mountain, in this slippery frozen lake env, there's a chance that the selected action is not taken hence the action probability here. In the grid world with mountain it was set to 1 (frozen lake can also be set to non slippery)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random policy\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA # equal prob on all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy_evaluation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-251b5dad884c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# evaluate the policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_policy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplot_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'policy_evaluation' is not defined"
     ]
    }
   ],
   "source": [
    "# evaluate the policy \n",
    "V = policy_evaluation(env, random_policy)\n",
    "\n",
    "plot_values(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test.run_check('policy_evaluation_check', policy_evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
