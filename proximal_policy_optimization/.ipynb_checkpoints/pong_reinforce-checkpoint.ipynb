{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sauce: \n",
    "- https://github.com/tnakae/Udacity-DeepRL-PPO/blob/master/pong-REINFORCE.ipynb\n",
    "- https://github.com/kurohi/deepreinforcement-udacity/tree/master/pong-reinforce\n",
    "- https://github.com/dolhana/udacity-deep-reinforcement-learning/blob/master/pong/pong-REINFORCE.ipynb\n",
    "- https://github.com/cwiz/Reinforcement_Learning-Policy_Gradients-2019/blob/master/pong-REINFORCE.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# custom utilies for displaying animation, collecting rollouts and more\n",
    "import pong_utils\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# check which device is being used. \n",
    "# I recommend disabling gpu until you've made sure that the code runs\n",
    "device = pong_utils.device\n",
    "print(\"using device: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "# render ai gym environment\n",
    "import gym\n",
    "import time\n",
    "\n",
    "# PongDeterministic does not contain random frameskip\n",
    "# so is faster to train than the vanilla Pong-v4 environment\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "\n",
    "print(\"List of available actions: \", env.unwrapped.get_action_meanings())\n",
    "\n",
    "# we will only use the actions 'RIGHTFIRE' = 4 and 'LEFTFIRE\" = 5\n",
    "# the 'FIRE' part ensures that the game starts again after losing a life\n",
    "# the actions are hard-coded in pong_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "To speed up training, we can simplify the input by cropping the images and use every other pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# show what a preprocessed image looks like\n",
    "env.reset()\n",
    "_, _, _, _ = env.step(0)\n",
    "# get a frame after 20 steps\n",
    "for _ in range(20):\n",
    "    frame, _, _, _ = env.step(1)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image')\n",
    "\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(pong_utils.preprocess_single(frame), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# set up a convolutional neural net\n",
    "# the output is the probability of moving right\n",
    "# P(left) = 1-P(right)\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        \n",
    "    ########\n",
    "    ## \n",
    "    ## Modify your neural network\n",
    "    ##\n",
    "    ########\n",
    "        \n",
    "        # 80x80 to outputsize x outputsize\n",
    "        # outputsize = (inputsize - kernel_size + stride)/stride \n",
    "        # (round up if not an integer)\n",
    "\n",
    "        # output = 20x20 here\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=4, stride=4)\n",
    "        self.size=1*20*20\n",
    "        \n",
    "        # 1 fully connected layer\n",
    "        self.fc = nn.Linear(self.size, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "    ########\n",
    "    ## \n",
    "    ## Modify your neural network\n",
    "    ##\n",
    "    ########\n",
    "    \n",
    "        x = F.relu(self.conv(x))\n",
    "        # flatten the tensor\n",
    "        x = x.view(-1,self.size)\n",
    "        return self.sig(self.fc(x))\n",
    "\n",
    "# use your own policy!\n",
    "# policy=Policy().to(device)\n",
    "\n",
    "\n",
    "policy=pong_utils.Policy().to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pong_utils.play(env, policy, time=100, preprocess=pong_utils.preprocess_single) \n",
    "# try to add the option \"preprocess=pong_utils.preprocess_single\"\n",
    "# to see what the agent sees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudpickle in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = pong_utils.parallelEnv('PongDeterministic-v4', n=8, seed=0)\n",
    "prob, state, action, reward = pong_utils.collect_trajectories(envs, policy, tmax=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_future_rewards(rewards, ratio=0.999):\n",
    "    n = rewards.shape[1]\n",
    "    step = torch.arange(n)[:,None] - torch.arange(n)[None,:]\n",
    "    ones = torch.ones_like(step)\n",
    "    zeros = torch.zeros_like(step)\n",
    "    \n",
    "    target = torch.where(step >= 0, ones, zeros)\n",
    "    step = torch.where(step >= 0, step, zeros)    \n",
    "    discount = target * (ratio ** step)\n",
    "    discount = discount.to(device)\n",
    "    \n",
    "    rewards_discounted = torch.mm(rewards, discount)\n",
    "    return rewards_discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"float\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-dd55b7153076>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mLsur\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msurrogate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLsur\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-dd55b7153076>\u001b[0m in \u001b[0;36msurrogate\u001b[1;34m(policy, old_probs, states, actions, rewards, discount, beta)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# this helps with exploration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# add in 1.e-10 to avoid log(0) which gives nan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n\u001b[0m\u001b[0;32m     23\u001b[0m         (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"float\") to list"
     ]
    }
   ],
   "source": [
    "def surrogate(policy, old_probs, states, actions, rewards,\n",
    "              discount = 0.995, beta=0.01):\n",
    "\n",
    "    ########\n",
    "    ## \n",
    "    ## WRITE YOUR OWN CODE HERE\n",
    "    ##\n",
    "    ########\n",
    "    \n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "\n",
    "    \n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = pong_utils.states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == pong_utils.RIGHT, new_probs, 1.0-new_probs)\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # which prevents policy to become exactly 0 or 1\n",
    "    # this helps with exploration\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    return torch.mean(beta*entropy)\n",
    "\n",
    "\n",
    "Lsur= surrogate(policy, prob, state, action, reward)\n",
    "\n",
    "print(Lsur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting progressbar\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/a6/b8e451f6cff1c99b4747a2f7235aa904d2d49e8e1464e0b798272aa84358/progressbar-2.5.tar.gz\n",
      "Building wheels for collected packages: progressbar\n",
      "  Building wheel for progressbar (setup.py): started\n",
      "  Building wheel for progressbar (setup.py): finished with status 'done'\n",
      "  Created wheel for progressbar: filename=progressbar-2.5-cp37-none-any.whl size=12078 sha256=af9bf02c9a238967c76742b7d35b3bc74931904607c36910b94316b547ab1c6b\n",
      "  Stored in directory: C:\\Users\\ASUS\\AppData\\Local\\pip\\Cache\\wheels\\c0\\e9\\6b\\ea01090205e285175842339aa3b491adeb4015206cda272ff0\n",
      "Successfully built progressbar\n",
      "Installing collected packages: progressbar\n",
      "Successfully installed progressbar-2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   1% |                                           | ETA:  1:36:14\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-47931190977b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# collect trajectories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mold_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mpong_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_trajectories\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mtotal_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\github\\Reinforcement-Learning\\proximal_policy_optimization\\pong_utils.py\u001b[0m in \u001b[0;36mcollect_trajectories\u001b[1;34m(envs, policy, tmax, nrand)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# shape (n, 2, 80, 80), the proper input for the policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# this is required when building CNN with pytorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mbatch_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfr1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfr2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;31m# probs will only be used as the pi_old\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\github\\Reinforcement-Learning\\proximal_policy_optimization\\pong_utils.py\u001b[0m in \u001b[0;36mpreprocess_batch\u001b[1;34m(images, bkg_color)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# subtract bkg and crop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     list_of_images_prepro = np.mean(list_of_images[:,:,34:-16:2,::2]-bkg_color,\n\u001b[1;32m---> 32\u001b[1;33m                                     axis=-1)/255.\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mbatch_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_of_images_prepro\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m   3116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3117\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[1;32m-> 3118\u001b[1;33m                           out=out, **kwargs)\n\u001b[0m\u001b[0;32m   3119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         ret = um.true_divide(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from parallelEnv import parallelEnv\n",
    "import numpy as np\n",
    "# WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "\n",
    "# training loop max iterations\n",
    "episode = 500\n",
    "# episode = 800\n",
    "\n",
    "# widget bar to display progress\n",
    "\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "# initialize environment\n",
    "envs = parallelEnv('PongDeterministic-v4', n=8, seed=1234)\n",
    "\n",
    "discount_rate = .99\n",
    "beta = .01\n",
    "tmax = 320\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = \\\n",
    "        pong_utils.collect_trajectories(envs, policy, tmax=tmax)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "    # this is the SOLUTION!\n",
    "    # use your own surrogate function\n",
    "    # L = -surrogate(policy, old_probs, states, actions, rewards, beta=beta)\n",
    "    \n",
    "    L = -pong_utils.surrogate(policy, old_probs, states, actions, rewards, beta=beta)\n",
    "    optimizer.zero_grad()\n",
    "    L.backward()\n",
    "    optimizer.step()\n",
    "    del L\n",
    "        \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
