{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling Double Deep Q-Network With Prioritized Experience Replay (Dueling DQN_PER)\n",
    "---\n",
    "\n",
    "Based on works from: [1](https://github.com/udacity/deep-reinforcement-learning), [2](https://github.com/andreidi/DDPG-TD3-PER/blob/master/per.py), [3](https://github.com/muhamuttaqien/AI-Lab/blob/master/deep-learning/reinforcement-learning/deep-q-learning/per-dqn/lunar-lander/pytorch-per-dqn.ipynb), [4](https://github.com/YijiongLin/Udacity_DRL_NanoDegree)\n",
    "\n",
    "#### Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools, operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Q-Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dueling_QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        super(Dueling_QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        # value layer\n",
    "        self.fc_vlayer = nn.Linear(fc2_units, 1)\n",
    "        # advantage layer\n",
    "        self.fc_alayer = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        value = self.fc_vlayer(x)\n",
    "        advantage = self.fc_alayer(x)\n",
    "        advantage_mean = advantage.mean(1).unsqueeze(1)\n",
    "        # Combine the value and advantage streams to final output.\n",
    "        # Normalized a with minus a.mean\n",
    "        Qsa = value + (advantage - advantage_mean)\n",
    "        return Qsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Prioritized Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaivePrioritizedReplayBuffer(object):\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        \n",
    "        self.alpha = 0.6                               # this constant controls the prioritization, (0: hard prioritization, 1: uniform sampling)\n",
    "        self.beta = 0.4                                # this constant controls the IS (Importance Sampling constant), (0: hard importance, 1: uniform importance)\n",
    "        self.epsilon = 0.01                              # this constant add small priority to avoid 0 opportunity experience to get taken\n",
    "        self.beta_increment_per_sampling = 0.001\n",
    "        self.absolute_error_upper = 1.                 # clipped abs error\n",
    "        \n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.priorities = deque(maxlen=buffer_size)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        max_priority = max(self.priorities) if self.memory else 1.0\n",
    "        \n",
    "        self.memory.append(e)\n",
    "        self.priorities.append(max_priority)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        priorities = np.array(self.priorities, dtype=np.float32)\n",
    "        probs = priorities ** self.alpha\n",
    "        probs/= probs.sum() \n",
    "        \n",
    "        indices = np.random.choice(len(self.memory), BATCH_SIZE, p=probs)\n",
    "        experiences = [self.memory[index] for index in indices]\n",
    "        \n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling]) \n",
    "        \n",
    "        total = len(self.memory)\n",
    "        weights = (total * probs[indices]) ** (-self.beta)\n",
    "        weights/= weights.max()\n",
    "        weights = np.array(weights, dtype=np.float32).reshape((-1,1))\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        weights = torch.from_numpy(weights).float().to(device)\n",
    "        \n",
    "        return indices, (states, actions, rewards, next_states, dones), weights\n",
    "    \n",
    "    def priority_batch_update(self, batch_indices, batch_priorities):\n",
    "        \"\"\"Update prioritied for every batch.\"\"\"\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = np.minimum(prio + 1e-5, 1.)[0]          # find out why the batch prio is in list, for now this should do\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        self.memory_size = BUFFER_SIZE\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = LR\n",
    "        \n",
    "        # Q-Network\n",
    "        self.qnetwork_local = Dueling_QNetwork(state_size, action_size, seed).to(device)              # some call this the expected network\n",
    "        self.qnetwork_target = Dueling_QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = NaivePrioritizedReplayBuffer(self.memory_size)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                indices_batch, experience_batch, is_weights = self.memory.sample()\n",
    "                self.learn(indices_batch, experience_batch, is_weights, GAMMA)                \n",
    "    \n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()                      # this change the local net to eval mode\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()                     # this just return the local net back to train mode\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, indices_batch, experience_batch, is_weights, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\"\"\"\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = experience_batch\n",
    "        \n",
    "        #################################################################################################################################################\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        qsa_next = self.qnetwork_local(next_states).detach()\n",
    "        _, action_max = qsa_next.max(1)\n",
    "        target_q_next = self.qnetwork_target(next_states).detach().gather(1, action_max.unsqueeze(1))                                   # double dqn part\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Vanilla DQN\n",
    "        target_q_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)       # this use epsilon greedy\n",
    "        \"\"\"\n",
    "        #################################################################################################################################################\n",
    "        \n",
    "        # Compute Q targets for current states \n",
    "        target_q = rewards+(gamma*target_q_next*(1-dones))\n",
    "                            \n",
    "        # Get expected Q values from local model\n",
    "        expected_q = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss using PER concept\n",
    "        loss  = torch.pow((expected_q - target_q), 2) * is_weights        \n",
    "        priorities = loss + self.memory.epsilon\n",
    "        \n",
    "        loss  = torch.mean(loss)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "                            \n",
    "        # Update Buffer Priorities\n",
    "        self.memory.priority_batch_update(indices_batch, priorities.data.cpu().numpy())\n",
    "\n",
    "            \n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (4,)\n",
      "Number of actions:  2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)\n",
    "\n",
    "agent = Agent(state_size=4, action_size=2, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the Environment and Agent\n",
    "Run the code cell below to train the agent from scratch.  You are welcome to amend the supplied values of the parameters in the function, to try to see if you can get better performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_per(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning with Naive Prioritized Experience Replay.\"\"\"\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "                \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 20.40\n",
      "Episode 200\tAverage Score: 14.39\n",
      "Episode 300\tAverage Score: 12.11\n",
      "Episode 400\tAverage Score: 11.83\n",
      "Episode 500\tAverage Score: 12.68\n",
      "Episode 600\tAverage Score: 14.18\n",
      "Episode 700\tAverage Score: 25.91\n",
      "Episode 800\tAverage Score: 120.37\n",
      "Episode 900\tAverage Score: 199.25\n",
      "Episode 907\tAverage Score: 200.03\n",
      "Environment solved in 807 episodes!\tAverage Score: 200.03\n"
     ]
    }
   ],
   "source": [
    "# train the agent\n",
    "scores = dqn_per(n_episodes=2000, max_t=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Emulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. using replay buffer\n",
    "2. using random walk + successful run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = list(agent.memory.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience(state=array([-0.04456399,  0.04653909,  0.01326909, -0.02099827]), action=1, reward=1.0, next_state=array([-0.04363321,  0.24146826,  0.01284913, -0.30946528]), done=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay buffer component\n",
    "state = pd.DataFrame([i[0] for i in np.array(replay_buffer)], columns=['S1','S2','S3','S4'])\n",
    "action = pd.DataFrame([i[1] for i in np.array(replay_buffer)], columns=['A'])\n",
    "reward = pd.DataFrame([i[2] for i in np.array(replay_buffer)], columns=['R'])\n",
    "next_state = pd.DataFrame([i[3] for i in np.array(replay_buffer)], columns=['NS1','NS2','NS3','NS4'])\n",
    "done = pd.DataFrame([i[4] for i in np.array(replay_buffer)], columns=['T'])\n",
    "\n",
    "# combined data\n",
    "df = pd.concat([state,action,reward,next_state,done],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "      <th>S4</th>\n",
       "      <th>A</th>\n",
       "      <th>R</th>\n",
       "      <th>NS1</th>\n",
       "      <th>NS2</th>\n",
       "      <th>NS3</th>\n",
       "      <th>NS4</th>\n",
       "      <th>T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.044564</td>\n",
       "      <td>0.046539</td>\n",
       "      <td>0.013269</td>\n",
       "      <td>-0.020998</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.043633</td>\n",
       "      <td>0.241468</td>\n",
       "      <td>0.012849</td>\n",
       "      <td>-0.309465</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.043633</td>\n",
       "      <td>0.241468</td>\n",
       "      <td>0.012849</td>\n",
       "      <td>-0.309465</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.038804</td>\n",
       "      <td>0.436405</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>-0.598068</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.038804</td>\n",
       "      <td>0.436405</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>-0.598068</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.030076</td>\n",
       "      <td>0.631433</td>\n",
       "      <td>-0.005302</td>\n",
       "      <td>-0.888646</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.030076</td>\n",
       "      <td>0.631433</td>\n",
       "      <td>-0.005302</td>\n",
       "      <td>-0.888646</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.017447</td>\n",
       "      <td>0.826626</td>\n",
       "      <td>-0.023074</td>\n",
       "      <td>-1.182991</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.017447</td>\n",
       "      <td>0.826626</td>\n",
       "      <td>-0.023074</td>\n",
       "      <td>-1.182991</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000915</td>\n",
       "      <td>0.631811</td>\n",
       "      <td>-0.046734</td>\n",
       "      <td>-0.897629</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         S1        S2        S3        S4  A    R       NS1       NS2  \\\n",
       "0 -0.044564  0.046539  0.013269 -0.020998  1  1.0 -0.043633  0.241468   \n",
       "1 -0.043633  0.241468  0.012849 -0.309465  1  1.0 -0.038804  0.436405   \n",
       "2 -0.038804  0.436405  0.006660 -0.598068  1  1.0 -0.030076  0.631433   \n",
       "3 -0.030076  0.631433 -0.005302 -0.888646  1  1.0 -0.017447  0.826626   \n",
       "4 -0.017447  0.826626 -0.023074 -1.182991  0  1.0 -0.000915  0.631811   \n",
       "\n",
       "        NS3       NS4      T  \n",
       "0  0.012849 -0.309465  False  \n",
       "1  0.006660 -0.598068  False  \n",
       "2 -0.005302 -0.888646  False  \n",
       "3 -0.023074 -1.182991  False  \n",
       "4 -0.046734 -0.897629  False  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data For Training\n",
    "\n",
    "df.loc[:,'T'] = df.loc[:,'T'].astype(float)\n",
    "\n",
    "x = torch.tensor(df.loc[:,['S1','S2','S3','S4','A']].values)\n",
    "y = torch.tensor(df.loc[:,['R','NS1','NS2','NS3','NS4','T']].values)\n",
    "                 \n",
    "# torch can only train on Variable, so convert them to Variable\n",
    "x, y = Variable(x), Variable(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = Data.TensorDataset(x, y)\n",
    "\n",
    "loader = Data.DataLoader(\n",
    "                            dataset=torch_dataset, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            shuffle=True, num_workers=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyper Parameter\n",
    "EPOCH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCNN(\n",
      "  (h1): Linear(in_features=5, out_features=300, bias=True)\n",
      "  (h2): Linear(in_features=300, out_features=200, bias=True)\n",
      "  (h3): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (h4): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (h5): Linear(in_features=50, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Fully Connected Network\n",
    "\n",
    "class FCNN(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.h1 = torch.nn.Linear(n_feature, 300)   \n",
    "        self.h2 = torch.nn.Linear(300, 200)    \n",
    "        self.h3 = torch.nn.Linear(200, 100)    \n",
    "        self.h4 = torch.nn.Linear(100, 50)     \n",
    "        self.h5 = torch.nn.Linear(50, n_output) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.h1(x))        # activation function for hidden layer\n",
    "        x = F.relu(self.h2(x))\n",
    "        x = F.relu(self.h3(x))\n",
    "        x = F.relu(self.h4(x))\n",
    "        x = F.relu(self.h5(x))\n",
    "        \n",
    "        x = self.predict(x)           # linear output\n",
    "        return x\n",
    "\n",
    "net = FCNN(n_feature=5, n_output=6)   # define the network\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "print(net)  # net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to define a network\n",
    "net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(5, 300),torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(300, 200),torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(200, 100),torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(100, 50),torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(50, 6),)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 200\n",
    "\n",
    "torch_dataset = Data.TensorDataset(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    for step, (batch_x, batch_y) in enumerate(loader): # for each training step\n",
    "        \n",
    "        b_x = Variable(batch_x)\n",
    "        b_y = Variable(batch_y)\n",
    "\n",
    "        prediction = net(b_x.float())     # input x and predict based on x\n",
    "\n",
    "        loss = loss_func(prediction, b_y.float())     # must be (1. nn output, 2. target)\n",
    "\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "\n",
    "#         if step == 1:\n",
    "#             # plot and show learning process\n",
    "#             plt.cla()\n",
    "#             ax.set_title('Regression Analysis - model 3 Batches', fontsize=35)\n",
    "#             ax.set_xlabel('Independent variable', fontsize=24)\n",
    "#             ax.set_ylabel('Dependent variable', fontsize=24)\n",
    "#             ax.set_xlim(-11.0, 13.0)\n",
    "#             ax.set_ylim(-1.1, 1.2)\n",
    "#             ax.scatter(b_x.data.numpy(), b_y.data.numpy(), color = \"blue\", alpha=0.2)\n",
    "#             ax.scatter(b_x.data.numpy(), prediction.data.numpy(), color='green', alpha=0.5)\n",
    "#             ax.text(8.8, -0.8, 'Epoch = %d' % epoch,\n",
    "#                     fontdict={'size': 24, 'color':  'red'})\n",
    "#             ax.text(8.8, -0.95, 'Loss = %.4f' % loss.data.numpy(),\n",
    "#                     fontdict={'size': 24, 'color':  'red'})\n",
    "\n",
    "#             # Used to return the plot as an image array \n",
    "#             # (https://ndres.me/post/matplotlib-animated-gifs-easily/)\n",
    "#             fig.canvas.draw()       # draw the canvas, cache the renderer\n",
    "#             image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "#             image  = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "#             my_images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred = net(torch_dataset[:][0].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "_act = torch_dataset[:][1].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0039, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(_pred,_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6278936360689314"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(_act.detach().numpy(),_pred.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.array(self.state), reward, done, {}\n",
    "\n",
    "env.reset()\n",
    "\n",
    "for i in range(1000000):\n",
    "    a = env.step(env.action_space.sample())\n",
    "    if a[1]>5:\n",
    "        print(i,a)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
