{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE_Pong\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will train REINFORCE with OpenAI Gym's Pong environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sauce: \n",
    "- https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity\n",
    "- https://github.com/tnakae/Udacity-DeepRL-PPO/blob/master/pong-REINFORCE.ipynb\n",
    "- https://github.com/kurohi/deepreinforcement-udacity/tree/master/pong-reinforce\n",
    "- https://github.com/dolhana/udacity-deep-reinforcement-learning/blob/master/pong/pong-REINFORCE.ipynb\n",
    "- https://github.com/cwiz/Reinforcement_Learning-Policy_Gradients-2019/blob/master/pong-REINFORCE.ipynb\n",
    "- https://github.com/a-windisch/deep_reinforcement_learning_play_pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudpickle in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: JSAnimation in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.1)\n",
      "Requirement already satisfied: progressbar in c:\\users\\asus\\anaconda3\\lib\\site-packages (2.5)\n"
     ]
    }
   ],
   "source": [
    "# install necessary packages\n",
    "!pip install cloudpickle\n",
    "!pip install JSAnimation\n",
    "!pip install progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from parallelEnv import parallelEnv \n",
    "\n",
    "import sys, time\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import progressbar as pb\n",
    "import random as rand\n",
    "\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "from  collections  import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available: False\n",
      "gpu count: 0\n",
      "gpu:\n"
     ]
    }
   ],
   "source": [
    "def check_gpu():\n",
    "    print(f'gpu available: {torch.cuda.is_available()}')\n",
    "    print(f'gpu count: {torch.cuda.device_count()}')\n",
    "    print(f'gpu:')\n",
    "    for no, i in enumerate(range(torch.cuda.device_count())):\n",
    "        print(f'  {no}: {torch.cuda.get_device_name(i)}')\n",
    "\n",
    "check_gpu()\n",
    "\n",
    "# check which device is being used. \n",
    "# I recommend disabling gpu until you've made sure that the code runs\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "# render ai gym environment\n",
    "\n",
    "# PongDeterministic does not contain random frameskip so is faster to train than the vanilla Pong-v4 environment\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "\n",
    "print(\"List of available actions: \", env.unwrapped.get_action_meanings())\n",
    "\n",
    "# we will only use the actions 'RIGHTFIRE' = 4 and 'LEFTFIRE\" = 5\n",
    "# the 'FIRE' part ensures that the game starts again after losing a life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the actions are hard-coded below\n",
    "RIGHT = 4\n",
    "LEFT = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "To speed up training, we can simplify the input by cropping the images and use every other pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess a single frame\n",
    "# crop image and downsample to 80x80, stack two frames together as input\n",
    "def preprocess_single(image, bkg_color = np.array([144, 72, 17])):\n",
    "    img = np.mean(image[34:-16:2,::2]-bkg_color, axis=-1)/255.\n",
    "    return img\n",
    "\n",
    "# convert outputs of parallelEnv to inputs to pytorch neural net\n",
    "# this is useful for batch processing especially on the GPU\n",
    "def preprocess_batch(images, bkg_color = np.array([144, 72, 17])):\n",
    "    list_of_images = np.asarray(images)\n",
    "    if len(list_of_images.shape) < 5:\n",
    "        list_of_images = np.expand_dims(list_of_images, 1)\n",
    "    # subtract bkg and crop\n",
    "    list_of_images_prepro = np.mean(list_of_images[:,:,34:-16:2,::2]-bkg_color,\n",
    "                                    axis=-1)/255.\n",
    "    batch_input = np.swapaxes(list_of_images_prepro,0,1)\n",
    "    return torch.from_numpy(batch_input).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show what a preprocessed image looks like\n",
    "env.reset()\n",
    "_, _, _, _ = env.step(0)\n",
    "# get a frame after 20 steps\n",
    "for _ in range(20):\n",
    "    frame, _, _, _ = env.step(1)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image')\n",
    "\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(preprocess_single(frame), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy\n",
    "\n",
    "## Exercise 1: Implement your policy\n",
    " \n",
    "Here, we define our policy. The input is the stack of two different frames (which captures the movement), and the output is a number $P_{\\rm right}$, the probability of moving left. Note that $P_{\\rm left}= 1-P_{\\rm right}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        # 80x80x2 to 38x38x4\n",
    "        # 2 channel from the stacked frame\n",
    "        # new_size = (size - kernel_size)/stride + 1, i.e. (80 - 6)/2 + 1 = 38\n",
    "        self.conv1 = nn.Conv2d(2, 4, kernel_size=6, stride=2, bias=False)\n",
    "        # 38x38x4 to 9x9x32\n",
    "        # new_size = (size - kernel_size)/stride + 1, i.e. (38 - 6)/4 + 1 = 9\n",
    "        self.conv2 = nn.Conv2d(4, 16, kernel_size=6, stride=4)\n",
    "        self.size=9*9*16\n",
    "        \n",
    "        # two fully connected layer\n",
    "        self.fc1 = nn.Linear(self.size, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "        # Sigmoid to \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1,self.size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.sig(self.fc2(x))\n",
    "    \n",
    "policy= Policy().to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4; optim.SGD is also possible\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game visualization\n",
    "pong_utils contain a play function given the environment and a policy. An optional preprocess function can be supplied. Here we define a function that plays a game and shows learning progress"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# function to animate a list of frames\n",
    "def animate_frames(frames):\n",
    "    plt.axis('off')\n",
    "\n",
    "    # color option for plotting\n",
    "    # use Greys for greyscale\n",
    "    cmap = None if len(frames[0].shape)==3 else 'Greys'\n",
    "    patch = plt.imshow(frames[0], cmap=cmap)  \n",
    "\n",
    "    fanim = animation.FuncAnimation(plt.gcf(), \\\n",
    "        lambda x: patch.set_data(frames[x]), frames = len(frames), interval=30)\n",
    "    \n",
    "    display(display_animation(fanim, default_mode='once'))\n",
    "\n",
    "# play a game and display the animation\n",
    "# nrand = number of random steps before using the policy\n",
    "def play(env, policy, time=2000, preprocess=None, nrand=5):\n",
    "    env.reset()\n",
    "\n",
    "    # star game\n",
    "    env.step(1)\n",
    "    \n",
    "    # perform nrand random steps in the beginning\n",
    "    for _ in range(nrand):\n",
    "        frame1, reward1, is_done, _ = env.step(np.random.choice([RIGHT,LEFT]))\n",
    "        frame2, reward2, is_done, _ = env.step(0)\n",
    "    \n",
    "    anim_frames = []\n",
    "    \n",
    "    for _ in range(time):\n",
    "        \n",
    "        frame_input = preprocess_batch([frame1, frame2])\n",
    "        prob = policy(frame_input)\n",
    "        \n",
    "        #a = rand.random()\n",
    "        #print('type a: ', type(a), ', type prob: ', type(prob), ', prob: ', prob)\n",
    "\n",
    "        # RIGHT = 4, LEFT = 5\n",
    "        action = RIGHT if rand.random() < prob else LEFT\n",
    "\n",
    "        frame1, _, is_done, _ = env.step(action)\n",
    "        frame2, _, is_done, _ = env.step(0)\n",
    "\n",
    "        if preprocess is None:\n",
    "            anim_frames.append(frame1)\n",
    "        else:\n",
    "            anim_frames.append(preprocess(frame1))\n",
    "\n",
    "        if is_done:\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    animate_frames(anim_frames)\n",
    "    return \n",
    "\n",
    "play(env, policy, time=100, preprocess=preprocess_single) \n",
    "# try to add the option \"preprocess=pong_utils.preprocess_single\"\n",
    "# to see what the agent sees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout\n",
    "Before we start the training, we need to collect samples. To make things efficient we use parallelized environments to collect multiple examples at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect trajectories for a parallelized parallelEnv object\n",
    "def collect_trajectories(envs, policy, tmax=200, nrand=5):\n",
    "    \n",
    "    # number of parallel instances\n",
    "    n=len(envs.ps)\n",
    "\n",
    "    #initialize returning lists and start the game!\n",
    "    state_list=[]\n",
    "    reward_list=[]\n",
    "    prob_list=[]\n",
    "    action_list=[]\n",
    "\n",
    "    envs.reset()\n",
    "    \n",
    "    # start all parallel agents\n",
    "    envs.step([1]*n)\n",
    "    \n",
    "    # perform nrand random steps\n",
    "    for _ in range(nrand):\n",
    "        fr1, re1, _, _ = envs.step(np.random.choice([RIGHT, LEFT],n))\n",
    "        fr2, re2, _, _ = envs.step([0]*n)\n",
    "    \n",
    "    for t in range(tmax):\n",
    "\n",
    "        # prepare the input\n",
    "        # preprocess_batch properly converts two frames into \n",
    "        # shape (n, 2, 80, 80), the proper input for the policy\n",
    "        # this is required when building CNN with pytorch\n",
    "        batch_input = preprocess_batch([fr1,fr2])\n",
    "        \n",
    "        # probs will only be used as the pi_old\n",
    "        # no gradient propagation is needed\n",
    "        # so we move it to the cpu\n",
    "        probs = policy(batch_input).squeeze().cpu().detach().numpy()\n",
    "        \n",
    "        action = np.where(np.random.rand(n) < probs, RIGHT, LEFT)\n",
    "        probs = np.where(action==RIGHT, probs, 1.0-probs)\n",
    "        \n",
    "        \n",
    "        # advance the game (0=no action)\n",
    "        # we take one action and skip game forward\n",
    "        fr1, re1, is_done, _ = envs.step(action)\n",
    "        fr2, re2, is_done, _ = envs.step([0]*n)\n",
    "\n",
    "        reward = re1 + re2\n",
    "        \n",
    "        # store the result\n",
    "        state_list.append(batch_input)\n",
    "        reward_list.append(reward)\n",
    "        prob_list.append(probs)\n",
    "        action_list.append(action)\n",
    "        \n",
    "        # stop if any of the trajectories is done\n",
    "        # we want all the lists to be retangular\n",
    "        if is_done.any():\n",
    "            break\n",
    "\n",
    "\n",
    "    # return pi_theta, states, actions, rewards, probability\n",
    "    return prob_list, state_list, action_list, reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = parallelEnv('PongDeterministic-v4', n=10, seed=0)\n",
    "prob, state, action, reward = collect_trajectories(envs, policy, tmax=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([-1.,  0., -1., -1., -1., -1., -1.,  0., -1., -1.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([-1.,  0., -1., -1., -1., -1., -1.,  0., -1., -1.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([-1.,  0., -1., -1.,  0., -1.,  0.,  0., -1., -1.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([ 0.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.]), array([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([-1.,  0., -1., -1.,  0., -1.,  0.,  0., -1., -1.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([ 0.,  0.,  0.,  0., -1.,  0.,  0., -1.,  0.,  0.]), array([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([-1.,  0., -1., -1.,  0., -1.,  0.,  0., -1., -1.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions\n",
    "Here you will define key functions for training. \n",
    "\n",
    "## Exercise 2: write your own function for training\n",
    "(this is the same as policy_loss except the negative sign)\n",
    "\n",
    "### REINFORCE\n",
    "you have two choices (usually it's useful to divide by the time since we've normalized our rewards and the time of each trajectory is fixed)\n",
    "\n",
    "1. $\\frac{1}{T}\\sum^T_t R_{t}^{\\rm future}\\log(\\pi_{\\theta'}(a_t|s_t))$\n",
    "2. $\\frac{1}{T}\\sum^T_t R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}$ where $\\theta'=\\theta$ and make sure that the no_grad is enabled when performing the division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "We are now ready to train our policy!\n",
    "WARNING: make sure to turn on GPU, which also enables multicore processing. It may take up to 45 minutes even with GPU enabled, otherwise it will take much longer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop max iterations\n",
    "episode = 2300 # 2000\n",
    "\n",
    "# initialize environment\n",
    "envs = parallelEnv('PongDeterministic-v4', n=10, seed=1234)\n",
    "\n",
    "# model hyperparameter\n",
    "discount_rate = .99\n",
    "beta = .01 \n",
    "tmax = 300 # 320-400 adviced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert states to probability, passing through the policy\n",
    "def states_to_prob(policy, states):\n",
    "    states = torch.stack(states)\n",
    "    policy_input = states.view(-1,*states.shape[-3:])\n",
    "    return policy(policy_input).view(states.shape[:-3])\n",
    "\n",
    "# return sum of log-prob divided by T\n",
    "# same thing as -policy_loss\n",
    "def surrogate(policy, old_probs, states, actions, rewards,\n",
    "              discount = 0.995, beta=0.01):\n",
    "\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    return torch.mean(ratio*rewards + beta*entropy)\n",
    "\n",
    "    \n",
    "# clipped surrogate function\n",
    "# similar as -policy_loss for REINFORCE, but for PPO\n",
    "def clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      discount=0.995,\n",
    "                      epsilon=0.1, beta=0.01):\n",
    "\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "    \n",
    "    # ratio for clipping\n",
    "    ratio = new_probs/old_probs\n",
    "    print(ratio)\n",
    "    # clipped function\n",
    "    clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "    \n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "#     import pdb; pdb.set_trace()\n",
    "    \n",
    "    # this returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T\n",
    "    # averaged over time-step and number of trajectories\n",
    "    # this is desirable because we have normalized our rewards\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<DivBackward0>)\n",
      "tensor([[0.9931, 0.9931, 1.0067,  ..., 1.0067, 1.0067, 0.9931],\n",
      "        [0.9932, 0.9932, 1.0067,  ..., 0.9930, 0.9931, 0.9930],\n",
      "        [0.9931, 0.9931, 0.9932,  ..., 1.0067, 0.9931, 0.9930],\n",
      "        ...,\n",
      "        [1.0068, 1.0066, 0.9931,  ..., 1.0067, 1.0066, 0.9930],\n",
      "        [1.0068, 0.9932, 1.0067,  ..., 0.9930, 0.9931, 0.9931],\n",
      "        [0.9930, 0.9932, 1.0067,  ..., 0.9930, 0.9930, 1.0068]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.9862, 0.9862, 1.0134,  ..., 1.0134, 1.0134, 0.9862],\n",
      "        [0.9863, 0.9863, 1.0134,  ..., 0.9861, 0.9863, 0.9862],\n",
      "        [0.9862, 0.9862, 0.9863,  ..., 1.0134, 0.9862, 0.9861],\n",
      "        ...,\n",
      "        [1.0136, 1.0132, 0.9863,  ..., 1.0135, 1.0132, 0.9862],\n",
      "        [1.0135, 0.9863, 1.0133,  ..., 0.9861, 0.9862, 0.9862],\n",
      "        [0.9861, 0.9864, 1.0134,  ..., 0.9861, 0.9861, 1.0135]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:43:25\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<DivBackward0>)\n",
      "tensor([[1.0008, 1.0007, 0.9993,  ..., 0.9993, 0.9993, 1.0008],\n",
      "        [0.9993, 0.9993, 0.9993,  ..., 0.9993, 1.0008, 1.0008],\n",
      "        [1.0008, 0.9993, 1.0008,  ..., 1.0008, 0.9993, 0.9993],\n",
      "        ...,\n",
      "        [0.9993, 0.9993, 1.0007,  ..., 0.9993, 0.9994, 0.9993],\n",
      "        [0.9993, 0.9993, 0.9992,  ..., 0.9993, 0.9993, 1.0007],\n",
      "        [0.9994, 0.9993, 0.9993,  ..., 1.0007, 0.9993, 1.0007]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[1.0039, 1.0039, 0.9964,  ..., 0.9964, 0.9964, 1.0039],\n",
      "        [0.9964, 0.9964, 0.9964,  ..., 0.9964, 1.0039, 1.0039],\n",
      "        [1.0039, 0.9964, 1.0039,  ..., 1.0039, 0.9964, 0.9964],\n",
      "        ...,\n",
      "        [0.9964, 0.9964, 1.0038,  ..., 0.9964, 0.9965, 0.9965],\n",
      "        [0.9964, 0.9964, 0.9963,  ..., 0.9964, 0.9964, 1.0037],\n",
      "        [0.9965, 0.9965, 0.9964,  ..., 1.0038, 0.9964, 1.0037]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:39:52\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<DivBackward0>)\n",
      "tensor([[0.9959, 1.0043, 1.0043,  ..., 0.9959, 0.9959, 1.0043],\n",
      "        [1.0043, 0.9959, 1.0043,  ..., 0.9959, 1.0043, 0.9959],\n",
      "        [1.0043, 1.0043, 0.9959,  ..., 0.9959, 0.9959, 1.0043],\n",
      "        ...,\n",
      "        [0.9959, 0.9959, 1.0043,  ..., 1.0043, 0.9959, 1.0043],\n",
      "        [1.0043, 0.9959, 1.0043,  ..., 0.9959, 1.0043, 1.0043],\n",
      "        [1.0043, 1.0043, 0.9959,  ..., 0.9959, 0.9959, 0.9959]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.9917, 1.0087, 1.0087,  ..., 0.9917, 0.9917, 1.0087],\n",
      "        [1.0087, 0.9917, 1.0087,  ..., 0.9917, 1.0087, 0.9917],\n",
      "        [1.0087, 1.0087, 0.9917,  ..., 0.9917, 0.9917, 1.0087],\n",
      "        ...,\n",
      "        [0.9917, 0.9917, 1.0087,  ..., 1.0087, 0.9917, 1.0087],\n",
      "        [1.0087, 0.9917, 1.0087,  ..., 0.9917, 1.0087, 1.0087],\n",
      "        [1.0087, 1.0087, 0.9917,  ..., 0.9917, 0.9917, 0.9917]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:46:42\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        ...,\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[1.0036, 0.9965, 0.9965,  ..., 0.9965, 1.0036, 0.9965],\n",
      "        [1.0036, 1.0036, 1.0036,  ..., 1.0036, 1.0036, 1.0036],\n",
      "        [0.9965, 1.0036, 1.0036,  ..., 0.9965, 0.9965, 1.0036],\n",
      "        ...,\n",
      "        [1.0036, 1.0036, 1.0036,  ..., 1.0036, 0.9965, 0.9965],\n",
      "        [1.0036, 1.0036, 1.0036,  ..., 1.0036, 0.9965, 1.0036],\n",
      "        [1.0036, 0.9965, 1.0036,  ..., 1.0036, 0.9965, 1.0036]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[1.0064, 0.9937, 0.9937,  ..., 0.9937, 1.0064, 0.9937],\n",
      "        [1.0064, 1.0064, 1.0064,  ..., 1.0064, 1.0064, 1.0064],\n",
      "        [0.9937, 1.0064, 1.0064,  ..., 0.9937, 0.9937, 1.0065],\n",
      "        ...,\n",
      "        [1.0065, 1.0064, 1.0065,  ..., 1.0064, 0.9937, 0.9937],\n",
      "        [1.0065, 1.0065, 1.0065,  ..., 1.0064, 0.9937, 1.0065],\n",
      "        [1.0065, 0.9938, 1.0065,  ..., 1.0065, 0.9937, 1.0065]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:52:46\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<DivBackward0>)\n",
      "tensor([[1.0029, 0.9971, 0.9971,  ..., 1.0029, 1.0029, 1.0029],\n",
      "        [0.9971, 0.9971, 0.9971,  ..., 1.0029, 1.0029, 0.9971],\n",
      "        [0.9971, 1.0029, 0.9971,  ..., 0.9972, 0.9971, 0.9971],\n",
      "        ...,\n",
      "        [0.9971, 1.0029, 0.9971,  ..., 1.0029, 1.0029, 0.9971],\n",
      "        [1.0029, 1.0029, 0.9971,  ..., 0.9971, 0.9971, 0.9971],\n",
      "        [0.9971, 1.0029, 0.9971,  ..., 1.0029, 1.0030, 1.0030]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[1.0063, 0.9937, 0.9937,  ..., 1.0063, 1.0063, 1.0063],\n",
      "        [0.9938, 0.9937, 0.9937,  ..., 1.0063, 1.0063, 0.9937],\n",
      "        [0.9938, 1.0063, 0.9938,  ..., 0.9938, 0.9938, 0.9938],\n",
      "        ...,\n",
      "        [0.9937, 1.0063, 0.9938,  ..., 1.0063, 1.0063, 0.9937],\n",
      "        [1.0063, 1.0063, 0.9938,  ..., 0.9938, 0.9937, 0.9937],\n",
      "        [0.9937, 1.0062, 0.9937,  ..., 1.0062, 1.0063, 1.0063]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:55:11\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<DivBackward0>)\n",
      "tensor([[0.9961, 0.9960, 0.9961,  ..., 1.0039, 1.0039, 1.0039],\n",
      "        [1.0039, 1.0039, 0.9961,  ..., 0.9961, 1.0039, 1.0039],\n",
      "        [1.0040, 1.0040, 0.9960,  ..., 0.9960, 1.0039, 0.9960],\n",
      "        ...,\n",
      "        [1.0039, 1.0040, 1.0040,  ..., 1.0039, 0.9961, 0.9960],\n",
      "        [1.0040, 0.9960, 1.0039,  ..., 0.9961, 0.9960, 1.0039],\n",
      "        [1.0039, 1.0040, 0.9961,  ..., 0.9960, 0.9961, 1.0038]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.9919, 0.9919, 0.9920,  ..., 1.0080, 1.0080, 1.0080],\n",
      "        [1.0080, 1.0080, 0.9919,  ..., 0.9919, 1.0080, 1.0080],\n",
      "        [1.0081, 1.0080, 0.9919,  ..., 0.9919, 1.0079, 0.9920],\n",
      "        ...,\n",
      "        [1.0079, 1.0081, 1.0080,  ..., 1.0079, 0.9920, 0.9919],\n",
      "        [1.0081, 0.9919, 1.0079,  ..., 0.9920, 0.9918, 1.0079],\n",
      "        [1.0080, 1.0081, 0.9920,  ..., 0.9919, 0.9920, 1.0079]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:54:44\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<DivBackward0>)\n",
      "tensor([[0.9956, 0.9956, 1.0043,  ..., 1.0042, 0.9956, 1.0043],\n",
      "        [0.9956, 0.9956, 0.9956,  ..., 1.0043, 1.0042, 0.9956],\n",
      "        [1.0043, 0.9956, 1.0043,  ..., 0.9956, 1.0043, 1.0043],\n",
      "        ...,\n",
      "        [1.0043, 0.9956, 0.9956,  ..., 1.0043, 0.9956, 1.0043],\n",
      "        [0.9955, 0.9956, 0.9956,  ..., 1.0042, 0.9955, 0.9956],\n",
      "        [1.0042, 1.0043, 0.9956,  ..., 0.9956, 1.0043, 0.9956]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.9911, 0.9911, 1.0086,  ..., 1.0086, 0.9910, 1.0086],\n",
      "        [0.9911, 0.9911, 0.9910,  ..., 1.0086, 1.0087, 0.9911],\n",
      "        [1.0086, 0.9910, 1.0087,  ..., 0.9910, 1.0087, 1.0087],\n",
      "        ...,\n",
      "        [1.0087, 0.9910, 0.9911,  ..., 1.0086, 0.9910, 1.0087],\n",
      "        [0.9910, 0.9911, 0.9910,  ..., 1.0086, 0.9910, 0.9911],\n",
      "        [1.0086, 1.0086, 0.9911,  ..., 0.9912, 1.0087, 0.9911]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:52:08\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<DivBackward0>)\n",
      "tensor([[1.0033, 1.0033, 0.9965,  ..., 1.0033, 0.9965, 0.9964],\n",
      "        [0.9965, 1.0033, 0.9965,  ..., 1.0033, 1.0033, 1.0033],\n",
      "        [1.0033, 0.9964, 0.9965,  ..., 0.9964, 1.0033, 0.9964],\n",
      "        ...,\n",
      "        [1.0033, 0.9965, 1.0033,  ..., 0.9965, 0.9965, 0.9965],\n",
      "        [0.9965, 0.9965, 0.9965,  ..., 0.9965, 1.0033, 1.0033],\n",
      "        [1.0034, 0.9964, 1.0033,  ..., 1.0033, 0.9965, 0.9965]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[1.0058, 1.0059, 0.9938,  ..., 1.0059, 0.9938, 0.9937],\n",
      "        [0.9937, 1.0059, 0.9938,  ..., 1.0059, 1.0059, 1.0058],\n",
      "        [1.0059, 0.9937, 0.9937,  ..., 0.9937, 1.0059, 0.9937],\n",
      "        ...,\n",
      "        [1.0058, 0.9938, 1.0059,  ..., 0.9938, 0.9937, 0.9938],\n",
      "        [0.9938, 0.9938, 0.9938,  ..., 0.9938, 1.0058, 1.0059],\n",
      "        [1.0060, 0.9936, 1.0058,  ..., 1.0058, 0.9937, 0.9938]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:51:12\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        ...,\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.9982, 1.0017, 0.9982,  ..., 0.9982, 0.9982, 1.0017],\n",
      "        [0.9982, 1.0017, 0.9982,  ..., 1.0017, 0.9982, 0.9982],\n",
      "        [1.0017, 1.0017, 1.0017,  ..., 1.0017, 1.0017, 0.9982],\n",
      "        ...,\n",
      "        [0.9982, 1.0017, 1.0017,  ..., 0.9982, 1.0017, 1.0017],\n",
      "        [1.0017, 0.9982, 0.9982,  ..., 1.0017, 1.0017, 0.9982],\n",
      "        [0.9982, 0.9982, 1.0017,  ..., 1.0017, 1.0017, 0.9982]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.9966, 1.0031, 0.9966,  ..., 0.9966, 0.9966, 1.0031],\n",
      "        [0.9966, 1.0031, 0.9966,  ..., 1.0031, 0.9966, 0.9966],\n",
      "        [1.0031, 1.0031, 1.0031,  ..., 1.0031, 1.0031, 0.9966],\n",
      "        ...,\n",
      "        [0.9966, 1.0031, 1.0031,  ..., 0.9966, 1.0031, 1.0031],\n",
      "        [1.0031, 0.9967, 0.9966,  ..., 1.0031, 1.0031, 0.9966],\n",
      "        [0.9966, 0.9966, 1.0031,  ..., 1.0031, 1.0032, 0.9966]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:49:02\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        ...,\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.9992, 1.0008, 1.0008,  ..., 0.9992, 1.0008, 1.0008],\n",
      "        [1.0008, 0.9992, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
      "        [1.0008, 0.9992, 1.0008,  ..., 0.9992, 1.0008, 0.9992],\n",
      "        ...,\n",
      "        [1.0008, 0.9991, 1.0008,  ..., 0.9992, 1.0008, 1.0008],\n",
      "        [1.0008, 1.0008, 0.9992,  ..., 0.9991, 0.9992, 1.0008],\n",
      "        [1.0008, 1.0008, 1.0008,  ..., 0.9992, 1.0008, 0.9991]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.9987, 1.0011, 1.0011,  ..., 0.9988, 1.0011, 1.0012],\n",
      "        [1.0011, 0.9988, 1.0011,  ..., 1.0011, 1.0011, 1.0011],\n",
      "        [1.0012, 0.9987, 1.0012,  ..., 0.9987, 1.0012, 0.9987],\n",
      "        ...,\n",
      "        [1.0011, 0.9987, 1.0012,  ..., 0.9987, 1.0011, 1.0011],\n",
      "        [1.0012, 1.0012, 0.9988,  ..., 0.9987, 0.9987, 1.0012],\n",
      "        [1.0012, 1.0012, 1.0011,  ..., 0.9988, 1.0012, 0.9987]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:46:50\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<DivBackward0>)\n",
      "tensor([[1.0001, 0.9999, 0.9999,  ..., 1.0001, 1.0001, 1.0001],\n",
      "        [0.9999, 1.0001, 0.9999,  ..., 0.9999, 1.0001, 0.9999],\n",
      "        [1.0001, 1.0001, 1.0001,  ..., 1.0001, 0.9999, 1.0000],\n",
      "        ...,\n",
      "        [0.9999, 0.9999, 0.9999,  ..., 0.9999, 0.9999, 0.9999],\n",
      "        [0.9999, 1.0001, 0.9999,  ..., 0.9999, 1.0001, 0.9999],\n",
      "        [1.0001, 0.9999, 0.9999,  ..., 1.0001, 0.9999, 1.0001]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[1.0002, 0.9998, 0.9998,  ..., 1.0002, 1.0002, 1.0002],\n",
      "        [0.9998, 1.0002, 0.9998,  ..., 0.9998, 1.0002, 0.9998],\n",
      "        [1.0002, 1.0002, 1.0002,  ..., 1.0002, 0.9998, 0.9998],\n",
      "        ...,\n",
      "        [0.9998, 0.9998, 0.9998,  ..., 0.9998, 0.9998, 0.9998],\n",
      "        [0.9998, 1.0002, 0.9998,  ..., 0.9998, 1.0002, 0.9998],\n",
      "        [1.0002, 0.9998, 0.9998,  ..., 1.0002, 0.9998, 1.0002]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:45:43\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        ...,\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.9996, 0.9996, 1.0005,  ..., 0.9996, 0.9996, 0.9996],\n",
      "        [0.9996, 1.0005, 1.0005,  ..., 1.0005, 1.0005, 1.0005],\n",
      "        [0.9996, 0.9996, 1.0005,  ..., 1.0005, 1.0005, 1.0005],\n",
      "        ...,\n",
      "        [0.9996, 1.0005, 1.0005,  ..., 1.0005, 0.9996, 0.9996],\n",
      "        [1.0005, 0.9996, 0.9996,  ..., 1.0005, 0.9996, 0.9996],\n",
      "        [1.0005, 1.0005, 1.0005,  ..., 0.9996, 1.0005, 0.9996]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.9989, 0.9989, 1.0012,  ..., 0.9989, 0.9989, 0.9989],\n",
      "        [0.9989, 1.0012, 1.0012,  ..., 1.0012, 1.0012, 1.0012],\n",
      "        [0.9989, 0.9989, 1.0012,  ..., 1.0012, 1.0012, 1.0012],\n",
      "        ...,\n",
      "        [0.9989, 1.0012, 1.0011,  ..., 1.0012, 0.9989, 0.9989],\n",
      "        [1.0012, 0.9989, 0.9990,  ..., 1.0012, 0.9989, 0.9989],\n",
      "        [1.0012, 1.0012, 1.0012,  ..., 0.9989, 1.0012, 0.9989]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:45:03\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        ...,\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.9988, 1.0013, 0.9988,  ..., 0.9988, 1.0013, 0.9988],\n",
      "        [1.0013, 1.0013, 0.9988,  ..., 1.0013, 0.9988, 1.0013],\n",
      "        [0.9988, 0.9988, 0.9988,  ..., 1.0013, 0.9988, 0.9988],\n",
      "        ...,\n",
      "        [0.9988, 1.0013, 0.9988,  ..., 0.9988, 1.0013, 0.9988],\n",
      "        [0.9988, 0.9988, 0.9988,  ..., 0.9988, 1.0013, 1.0013],\n",
      "        [0.9988, 0.9988, 1.0013,  ..., 0.9988, 1.0013, 0.9988]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.9973, 1.0029, 0.9973,  ..., 0.9973, 1.0029, 0.9973],\n",
      "        [1.0029, 1.0029, 0.9973,  ..., 1.0029, 0.9973, 1.0029],\n",
      "        [0.9973, 0.9973, 0.9973,  ..., 1.0029, 0.9973, 0.9973],\n",
      "        ...,\n",
      "        [0.9973, 1.0029, 0.9973,  ..., 0.9973, 1.0029, 0.9973],\n",
      "        [0.9973, 0.9973, 0.9973,  ..., 0.9973, 1.0029, 1.0029],\n",
      "        [0.9973, 0.9973, 1.0029,  ..., 0.9973, 1.0029, 0.9973]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:43:35\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<DivBackward0>)\n",
      "tensor([[1.0018, 0.9983, 0.9983,  ..., 1.0018, 1.0018, 0.9983],\n",
      "        [0.9983, 1.0018, 0.9983,  ..., 0.9983, 0.9983, 0.9983],\n",
      "        [1.0018, 0.9983, 0.9983,  ..., 1.0018, 0.9983, 1.0018],\n",
      "        ...,\n",
      "        [0.9983, 0.9983, 1.0018,  ..., 1.0018, 1.0018, 0.9983],\n",
      "        [1.0018, 0.9983, 1.0018,  ..., 0.9983, 0.9983, 1.0018],\n",
      "        [0.9983, 0.9983, 1.0018,  ..., 1.0018, 1.0018, 1.0018]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[1.0035, 0.9967, 0.9967,  ..., 1.0035, 1.0035, 0.9968],\n",
      "        [0.9967, 1.0035, 0.9967,  ..., 0.9967, 0.9968, 0.9968],\n",
      "        [1.0035, 0.9967, 0.9967,  ..., 1.0035, 0.9967, 1.0035],\n",
      "        ...,\n",
      "        [0.9967, 0.9967, 1.0035,  ..., 1.0035, 1.0035, 0.9967],\n",
      "        [1.0035, 0.9967, 1.0035,  ..., 0.9967, 0.9967, 1.0035],\n",
      "        [0.9967, 0.9967, 1.0035,  ..., 1.0035, 1.0035, 1.0035]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:42:08\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<DivBackward0>)\n",
      "tensor([[1.0009, 1.0009, 0.9992,  ..., 1.0009, 0.9992, 1.0009],\n",
      "        [1.0009, 0.9991, 1.0009,  ..., 0.9991, 0.9992, 0.9991],\n",
      "        [1.0009, 0.9992, 1.0009,  ..., 1.0009, 0.9992, 0.9991],\n",
      "        ...,\n",
      "        [0.9991, 0.9992, 0.9991,  ..., 0.9992, 0.9991, 1.0009],\n",
      "        [0.9992, 0.9991, 1.0009,  ..., 0.9992, 1.0009, 1.0009],\n",
      "        [1.0009, 1.0009, 0.9991,  ..., 0.9991, 1.0009, 0.9991]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[1.0012, 1.0012, 0.9989,  ..., 1.0012, 0.9989, 1.0012],\n",
      "        [1.0012, 0.9989, 1.0012,  ..., 0.9989, 0.9989, 0.9989],\n",
      "        [1.0012, 0.9989, 1.0012,  ..., 1.0012, 0.9989, 0.9989],\n",
      "        ...,\n",
      "        [0.9989, 0.9989, 0.9989,  ..., 0.9989, 0.9989, 1.0012],\n",
      "        [0.9989, 0.9989, 1.0012,  ..., 0.9989, 1.0012, 1.0012],\n",
      "        [1.0012, 1.0012, 0.9989,  ..., 0.9989, 1.0012, 0.9989]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:42:31\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<DivBackward0>)\n",
      "tensor([[1.0003, 0.9997, 1.0003,  ..., 0.9997, 1.0003, 0.9997],\n",
      "        [0.9997, 0.9997, 1.0003,  ..., 0.9997, 0.9997, 0.9997],\n",
      "        [1.0003, 0.9997, 1.0003,  ..., 0.9997, 1.0003, 1.0003],\n",
      "        ...,\n",
      "        [1.0003, 0.9997, 1.0003,  ..., 1.0003, 1.0003, 1.0003],\n",
      "        [1.0003, 0.9997, 1.0003,  ..., 0.9997, 1.0003, 0.9997],\n",
      "        [0.9997, 0.9997, 1.0003,  ..., 1.0003, 1.0003, 1.0003]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[1.0007, 0.9993, 1.0007,  ..., 0.9993, 1.0007, 0.9993],\n",
      "        [0.9993, 0.9993, 1.0007,  ..., 0.9993, 0.9993, 0.9993],\n",
      "        [1.0007, 0.9993, 1.0007,  ..., 0.9993, 1.0007, 1.0007],\n",
      "        ...,\n",
      "        [1.0007, 0.9993, 1.0007,  ..., 1.0007, 1.0007, 1.0007],\n",
      "        [1.0007, 0.9993, 1.0007,  ..., 0.9993, 1.0007, 0.9993],\n",
      "        [0.9993, 0.9993, 1.0007,  ..., 1.0007, 1.0007, 1.0007]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:42:19\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<DivBackward0>)\n",
      "tensor([[0.9991, 1.0008, 0.9991,  ..., 0.9991, 1.0008, 1.0008],\n",
      "        [1.0008, 0.9991, 0.9991,  ..., 1.0008, 1.0008, 1.0008],\n",
      "        [0.9991, 0.9991, 1.0008,  ..., 1.0008, 0.9991, 1.0008],\n",
      "        ...,\n",
      "        [0.9991, 1.0008, 0.9991,  ..., 1.0008, 0.9991, 1.0008],\n",
      "        [1.0008, 1.0008, 0.9991,  ..., 1.0008, 1.0008, 1.0008],\n",
      "        [0.9991, 1.0008, 1.0008,  ..., 1.0008, 0.9991, 1.0008]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.9979, 1.0020, 0.9979,  ..., 0.9979, 1.0020, 1.0020],\n",
      "        [1.0020, 0.9979, 0.9979,  ..., 1.0020, 1.0020, 1.0020],\n",
      "        [0.9979, 0.9979, 1.0020,  ..., 1.0020, 0.9979, 1.0020],\n",
      "        ...,\n",
      "        [0.9979, 1.0020, 0.9979,  ..., 1.0020, 0.9979, 1.0020],\n",
      "        [1.0020, 1.0020, 0.9979,  ..., 1.0020, 1.0020, 1.0020],\n",
      "        [0.9979, 1.0020, 1.0020,  ..., 1.0020, 0.9979, 1.0020]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:42:06\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<DivBackward0>)\n",
      "tensor([[0.9987, 1.0012, 1.0012,  ..., 1.0012, 0.9987, 0.9987],\n",
      "        [1.0012, 1.0012, 0.9987,  ..., 1.0012, 1.0012, 0.9987],\n",
      "        [1.0013, 1.0013, 0.9987,  ..., 0.9987, 1.0013, 1.0012],\n",
      "        ...,\n",
      "        [0.9987, 1.0013, 0.9987,  ..., 0.9987, 1.0013, 0.9987],\n",
      "        [0.9987, 1.0013, 0.9986,  ..., 0.9987, 1.0013, 0.9986],\n",
      "        [1.0012, 1.0013, 0.9986,  ..., 1.0013, 0.9987, 1.0013]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.9975, 1.0023, 1.0023,  ..., 1.0023, 0.9975, 0.9975],\n",
      "        [1.0023, 1.0023, 0.9975,  ..., 1.0023, 1.0023, 0.9975],\n",
      "        [1.0023, 1.0023, 0.9975,  ..., 0.9975, 1.0023, 1.0023],\n",
      "        ...,\n",
      "        [0.9975, 1.0023, 0.9975,  ..., 0.9975, 1.0023, 0.9975],\n",
      "        [0.9975, 1.0023, 0.9975,  ..., 0.9975, 1.0023, 0.9975],\n",
      "        [1.0023, 1.0023, 0.9975,  ..., 1.0023, 0.9975, 1.0023]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:41:12\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<DivBackward0>)\n",
      "tensor([[0.9985, 1.0014, 1.0014,  ..., 1.0014, 0.9985, 1.0014],\n",
      "        [1.0014, 0.9985, 1.0014,  ..., 1.0014, 1.0014, 1.0014],\n",
      "        [0.9985, 1.0014, 0.9985,  ..., 1.0014, 0.9985, 0.9985],\n",
      "        ...,\n",
      "        [0.9985, 0.9985, 0.9985,  ..., 1.0014, 0.9985, 1.0014],\n",
      "        [1.0014, 1.0014, 1.0014,  ..., 0.9985, 1.0014, 1.0014],\n",
      "        [0.9985, 0.9985, 1.0014,  ..., 0.9985, 1.0014, 0.9985]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[0.9965, 1.0032, 1.0032,  ..., 1.0032, 0.9965, 1.0032],\n",
      "        [1.0032, 0.9965, 1.0032,  ..., 1.0032, 1.0032, 1.0032],\n",
      "        [0.9965, 1.0032, 0.9965,  ..., 1.0032, 0.9965, 0.9965],\n",
      "        ...,\n",
      "        [0.9965, 0.9965, 0.9965,  ..., 1.0032, 0.9965, 1.0032],\n",
      "        [1.0032, 1.0032, 1.0032,  ..., 0.9965, 1.0032, 1.0032],\n",
      "        [0.9965, 0.9965, 1.0032,  ..., 0.9965, 1.0032, 0.9965]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:40:25\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        ...,\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[1.0023, 1.0023, 0.9974,  ..., 1.0023, 1.0023, 0.9974],\n",
      "        [1.0023, 0.9974, 0.9975,  ..., 0.9975, 1.0023, 0.9974],\n",
      "        [0.9974, 1.0023, 1.0023,  ..., 1.0023, 0.9974, 1.0023],\n",
      "        ...,\n",
      "        [0.9974, 1.0023, 0.9974,  ..., 0.9974, 1.0023, 0.9974],\n",
      "        [0.9974, 1.0023, 1.0023,  ..., 1.0023, 0.9975, 0.9974],\n",
      "        [1.0023, 0.9975, 0.9975,  ..., 1.0023, 1.0023, 0.9974]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[1.0047, 1.0047, 0.9949,  ..., 1.0047, 1.0046, 0.9949],\n",
      "        [1.0046, 0.9949, 0.9949,  ..., 0.9949, 1.0047, 0.9949],\n",
      "        [0.9949, 1.0047, 1.0047,  ..., 1.0047, 0.9949, 1.0047],\n",
      "        ...,\n",
      "        [0.9949, 1.0047, 0.9948,  ..., 0.9949, 1.0047, 0.9949],\n",
      "        [0.9948, 1.0047, 1.0047,  ..., 1.0047, 0.9949, 0.9948],\n",
      "        [1.0046, 0.9950, 0.9949,  ..., 1.0047, 1.0047, 0.9949]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:39:23\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<DivBackward0>)\n",
      "tensor([[1.0020, 0.9978, 1.0020,  ..., 0.9978, 1.0020, 1.0020],\n",
      "        [0.9978, 1.0020, 0.9978,  ..., 0.9978, 1.0020, 0.9978],\n",
      "        [0.9978, 1.0020, 1.0020,  ..., 1.0020, 1.0020, 0.9978],\n",
      "        ...,\n",
      "        [0.9978, 1.0020, 1.0020,  ..., 0.9978, 0.9978, 1.0020],\n",
      "        [0.9978, 0.9978, 0.9978,  ..., 1.0020, 1.0020, 1.0020],\n",
      "        [1.0020, 1.0020, 1.0019,  ..., 1.0020, 0.9978, 1.0020]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[1.0036, 0.9959, 1.0036,  ..., 0.9959, 1.0036, 1.0037],\n",
      "        [0.9960, 1.0037, 0.9960,  ..., 0.9959, 1.0037, 0.9959],\n",
      "        [0.9960, 1.0037, 1.0036,  ..., 1.0036, 1.0036, 0.9959],\n",
      "        ...,\n",
      "        [0.9959, 1.0037, 1.0036,  ..., 0.9960, 0.9959, 1.0036],\n",
      "        [0.9960, 0.9960, 0.9960,  ..., 1.0036, 1.0037, 1.0036],\n",
      "        [1.0036, 1.0036, 1.0036,  ..., 1.0036, 0.9959, 1.0037]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:38:43\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        ...,\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "scores_deque = deque(maxlen=100)\n",
    "\n",
    "# progress bar\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "for e in range(episode):\n",
    "    \n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = collect_trajectories(envs, policy, tmax=tmax)\n",
    "    \n",
    "    # discount the rewards\n",
    "#     rewards = discounted_future_rewards(rewards, ratio=0.999)\n",
    "    \n",
    "    # get total rewards\n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "    scores_deque.append(total_rewards)\n",
    "    \n",
    "    # this is the SOLUTION!\n",
    "    # use your own surrogate function\n",
    "    L = -clipped_surrogate(policy, old_probs, states, actions, rewards, beta=beta) # minus because default in pytorch in gradient descent, we wanted gradient ascent here\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    L.backward()\n",
    "    optimizer.step()\n",
    "    del L\n",
    "        \n",
    "    # the regulation term also reduces, this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 100 iterations\n",
    "    if (e+1)%100 ==0 :\n",
    "        print(\"Episode: {}, \\tscore: {:.2f} \\tAverage Score: {:.2f}\" \\\n",
    "               .format(e+1, np.mean(total_rewards), np.mean(scores_deque)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# play game after training!\n",
    "play(env, model, time=2000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Sigmoid()\n",
    "a(torch.tensor([-10, 3.,5.,10.,100.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mean rewards\n",
    "plt.plot(mean_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save agent\n",
    "# torch.save(model.state_dict(), 'surrogate_00003.policy')\n",
    "# torch.save(model.state_dict(), 'surrogate_clipped_00002.policy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
