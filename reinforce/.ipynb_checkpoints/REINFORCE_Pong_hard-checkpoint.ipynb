{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE_Pong_Hard\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will train REINFORCE with OpenAI Gym's Pong environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sauce: \n",
    "- https://github.com/Rafael1s/Deep-Reinforcement-Learning-Udacity\n",
    "- https://github.com/tnakae/Udacity-DeepRL-PPO/blob/master/pong-REINFORCE.ipynb\n",
    "- https://github.com/kurohi/deepreinforcement-udacity/tree/master/pong-reinforce\n",
    "- https://github.com/dolhana/udacity-deep-reinforcement-learning/blob/master/pong/pong-REINFORCE.ipynb\n",
    "- https://github.com/cwiz/Reinforcement_Learning-Policy_Gradients-2019/blob/master/pong-REINFORCE.ipynb\n",
    "- https://github.com/a-windisch/deep_reinforcement_learning_play_pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (1.2.2)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: JSAnimation in /usr/local/lib/python3.6/dist-packages (0.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: progressbar in /usr/local/lib/python3.6/dist-packages (2.5)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install necessary packages\n",
    "!pip install cloudpickle\n",
    "!pip install JSAnimation\n",
    "!pip install progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from parallelEnv import parallelEnv\n",
    "\n",
    "import sys, time\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import progressbar as pb\n",
    "import random as rand\n",
    "\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "from collections  import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available: True\n",
      "gpu count: 1\n",
      "gpu:\n",
      "  0: TITAN V\n"
     ]
    }
   ],
   "source": [
    "def check_gpu():\n",
    "    print(f'gpu available: {torch.cuda.is_available()}')\n",
    "    print(f'gpu count: {torch.cuda.device_count()}')\n",
    "    print(f'gpu:')\n",
    "    for no, i in enumerate(range(torch.cuda.device_count())):\n",
    "        print(f'  {no}: {torch.cuda.get_device_name(i)}')\n",
    "\n",
    "check_gpu()\n",
    "\n",
    "# check which device is being used. \n",
    "# I recommend disabling gpu until you've made sure that the code runs\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "# render ai gym environment\n",
    "\n",
    "# PongDeterministic does not contain random frameskip so is faster to train than the vanilla Pong-v4 environment\n",
    "env = gym.make('Pong-v4')\n",
    "\n",
    "print(\"List of available actions: \", env.unwrapped.get_action_meanings())\n",
    "\n",
    "# we will only use the actions 'RIGHTFIRE' = 4 and 'LEFTFIRE\" = 5\n",
    "# the 'FIRE' part ensures that the game starts again after losing a life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the actions are hard-coded below\n",
    "RIGHT = 4\n",
    "LEFT = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "To speed up training, we can simplify the input by cropping the images and use every other pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess a single frame\n",
    "# crop image and downsample to 80x80, stack two frames together as input\n",
    "def preprocess_single(image, bkg_color = np.array([144, 72, 17])):\n",
    "    img = np.mean(image[34:-16:2,::2]-bkg_color, axis=-1)/255.\n",
    "    return img\n",
    "\n",
    "# convert outputs of parallelEnv to inputs to pytorch neural net\n",
    "# this is useful for batch processing especially on the GPU\n",
    "def preprocess_batch(images, bkg_color = np.array([144, 72, 17])):\n",
    "    list_of_images = np.asarray(images)\n",
    "    if len(list_of_images.shape) < 5:\n",
    "        list_of_images = np.expand_dims(list_of_images, 1)\n",
    "    # subtract bkg and crop\n",
    "    list_of_images_prepro = np.mean(list_of_images[:,:,34:-16:2,::2]-bkg_color,\n",
    "                                    axis=-1)/255.\n",
    "    batch_input = np.swapaxes(list_of_images_prepro,0,1)\n",
    "    return torch.from_numpy(batch_input).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show what a preprocessed image looks like\n",
    "env.reset()\n",
    "_, _, _, _ = env.step(0)\n",
    "# get a frame after 20 steps\n",
    "for _ in range(20):\n",
    "    frame, _, _, _ = env.step(1)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image')\n",
    "\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(preprocess_single(frame), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy\n",
    "\n",
    "## Exercise 1: Implement your policy\n",
    " \n",
    "Here, we define our policy. The input is the stack of two different frames (which captures the movement), and the output is a number $P_{\\rm right}$, the probability of moving left. Note that $P_{\\rm left}= 1-P_{\\rm right}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        # 80x80x2 to 38x38x4\n",
    "        # 2 channel from the stacked frame\n",
    "        # new_size = (size - kernel_size)/stride + 1, i.e. (80 - 6)/2 + 1 = 38\n",
    "        self.conv1 = nn.Conv2d(2, 4, kernel_size=6, stride=2, bias=False)\n",
    "        # 38x38x4 to 9x9x32\n",
    "        # new_size = (size - kernel_size)/stride + 1, i.e. (38 - 6)/4 + 1 = 9\n",
    "        self.conv2 = nn.Conv2d(4, 16, kernel_size=6, stride=4)\n",
    "        self.size=9*9*16\n",
    "        \n",
    "        # two fully connected layer\n",
    "        self.fc1 = nn.Linear(self.size, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "        # Sigmoid to \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1,self.size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.sig(self.fc2(x))\n",
    "    \n",
    "policy= Policy().to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4; optim.SGD is also possible\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game visualization\n",
    "pong_utils contain a play function given the environment and a policy. An optional preprocess function can be supplied. Here we define a function that plays a game and shows learning progress"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# function to animate a list of frames\n",
    "def animate_frames(frames):\n",
    "    plt.axis('off')\n",
    "\n",
    "    # color option for plotting\n",
    "    # use Greys for greyscale\n",
    "    cmap = None if len(frames[0].shape)==3 else 'Greys'\n",
    "    patch = plt.imshow(frames[0], cmap=cmap)  \n",
    "\n",
    "    fanim = animation.FuncAnimation(plt.gcf(), \\\n",
    "        lambda x: patch.set_data(frames[x]), frames = len(frames), interval=30)\n",
    "    \n",
    "    display(display_animation(fanim, default_mode='once'))\n",
    "\n",
    "# play a game and display the animation\n",
    "# nrand = number of random steps before using the policy\n",
    "def play(env, policy, time=2000, preprocess=None, nrand=5):\n",
    "    env.reset()\n",
    "\n",
    "    # star game\n",
    "    env.step(1)\n",
    "    \n",
    "    # perform nrand random steps in the beginning\n",
    "    for _ in range(nrand):\n",
    "        frame1, reward1, is_done, _ = env.step(np.random.choice([RIGHT,LEFT]))\n",
    "        frame2, reward2, is_done, _ = env.step(0)\n",
    "    \n",
    "    anim_frames = []\n",
    "    \n",
    "    for _ in range(time):\n",
    "        \n",
    "        frame_input = preprocess_batch([frame1, frame2])\n",
    "        prob = policy(frame_input)\n",
    "        \n",
    "        #a = rand.random()\n",
    "        #print('type a: ', type(a), ', type prob: ', type(prob), ', prob: ', prob)\n",
    "\n",
    "        # RIGHT = 4, LEFT = 5\n",
    "        action = RIGHT if rand.random() < prob else LEFT\n",
    "\n",
    "        frame1, _, is_done, _ = env.step(action)\n",
    "        frame2, _, is_done, _ = env.step(0)\n",
    "\n",
    "        if preprocess is None:\n",
    "            anim_frames.append(frame1)\n",
    "        else:\n",
    "            anim_frames.append(preprocess(frame1))\n",
    "\n",
    "        if is_done:\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    animate_frames(anim_frames)\n",
    "    return \n",
    "\n",
    "play(env, policy, time=100, preprocess=preprocess_single) \n",
    "# try to add the option \"preprocess=pong_utils.preprocess_single\"\n",
    "# to see what the agent sees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout\n",
    "Before we start the training, we need to collect samples. To make things efficient we use parallelized environments to collect multiple examples at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect trajectories for a parallelized parallelEnv object\n",
    "def collect_trajectories(envs, policy, tmax=200, nrand=5):\n",
    "    \n",
    "    # number of parallel instances\n",
    "    n=len(envs.ps)\n",
    "\n",
    "    #initialize returning lists and start the game!\n",
    "    state_list=[]\n",
    "    reward_list=[]\n",
    "    prob_list=[]\n",
    "    action_list=[]\n",
    "\n",
    "    envs.reset()\n",
    "    \n",
    "    # start all parallel agents\n",
    "    envs.step([1]*n)\n",
    "    \n",
    "    # perform nrand random steps\n",
    "    for _ in range(nrand):\n",
    "        fr1, re1, _, _ = envs.step(np.random.choice([RIGHT, LEFT],n))\n",
    "        fr2, re2, _, _ = envs.step([0]*n)\n",
    "    \n",
    "    for t in range(tmax):\n",
    "\n",
    "        # prepare the input\n",
    "        # preprocess_batch properly converts two frames into \n",
    "        # shape (n, 2, 80, 80), the proper input for the policy\n",
    "        # this is required when building CNN with pytorch\n",
    "        batch_input = preprocess_batch([fr1,fr2])\n",
    "        \n",
    "        # probs will only be used as the pi_old\n",
    "        # no gradient propagation is needed\n",
    "        # so we move it to the cpu\n",
    "        probs = policy(batch_input).squeeze().cpu().detach().numpy()\n",
    "        \n",
    "        action = np.where(np.random.rand(n) < probs, RIGHT, LEFT)\n",
    "        probs = np.where(action==RIGHT, probs, 1.0-probs)\n",
    "        \n",
    "        \n",
    "        # advance the game (0=no action)\n",
    "        # we take one action and skip game forward\n",
    "        fr1, re1, is_done, _ = envs.step(action)\n",
    "        fr2, re2, is_done, _ = envs.step([0]*n)\n",
    "\n",
    "        reward = re1 + re2\n",
    "        \n",
    "        # store the result\n",
    "        state_list.append(batch_input)\n",
    "        reward_list.append(reward)\n",
    "        prob_list.append(probs)\n",
    "        action_list.append(action)\n",
    "        \n",
    "        # stop if any of the trajectories is done\n",
    "        # we want all the lists to be retangular\n",
    "        if is_done.any():\n",
    "            break\n",
    "\n",
    "\n",
    "    # return pi_theta, states, actions, rewards, probability\n",
    "    return prob_list, state_list, action_list, reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = parallelEnv('PongDeterministic-v4', n=8, seed=0)\n",
    "prob, state, action, reward = collect_trajectories(envs, policy, tmax=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([-1., -1.,  0., -1., -1., -1., -1., -1.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([-1., -1.,  0., -1., -1., -1., -1.,  0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([-1., -1.,  0., -1., -1., -1., -1.,  0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([-1., -1.,  0., -1., -1., -1., -1.,  0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.]), array([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([-1., -1.,  0., -1., -1., -1., -1.,  0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions\n",
    "Here you will define key functions for training. \n",
    "\n",
    "## Exercise 2: write your own function for training\n",
    "(this is the same as policy_loss except the negative sign)\n",
    "\n",
    "### REINFORCE\n",
    "you have two choices (usually it's useful to divide by the time since we've normalized our rewards and the time of each trajectory is fixed)\n",
    "\n",
    "1. $\\frac{1}{T}\\sum^T_t R_{t}^{\\rm future}\\log(\\pi_{\\theta'}(a_t|s_t))$\n",
    "2. $\\frac{1}{T}\\sum^T_t R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}$ where $\\theta'=\\theta$ and make sure that the no_grad is enabled when performing the division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "We are now ready to train our policy!\n",
    "WARNING: make sure to turn on GPU, which also enables multicore processing. It may take up to 45 minutes even with GPU enabled, otherwise it will take much longer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop max iterations\n",
    "episode = 2300 # 2000\n",
    "\n",
    "# initialize environment\n",
    "envs = parallelEnv('PongDeterministic-v4', n=8, seed=1234)\n",
    "\n",
    "# model hyperparameter\n",
    "discount_rate = .99\n",
    "beta = .01 \n",
    "tmax = 300 # 320-400 adviced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert states to probability, passing through the policy\n",
    "def states_to_prob(policy, states):\n",
    "    states = torch.stack(states)\n",
    "    policy_input = states.view(-1,*states.shape[-3:])\n",
    "    return policy(policy_input).view(states.shape[:-3])\n",
    "\n",
    "# return sum of log-prob divided by T\n",
    "# same thing as -policy_loss\n",
    "def surrogate(policy, old_probs, states, actions, rewards,\n",
    "              discount = 0.995, beta=0.01):\n",
    "\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    return torch.mean(ratio*rewards + beta*entropy)\n",
    "\n",
    "    \n",
    "# clipped surrogate function\n",
    "# similar as -policy_loss for REINFORCE, but for PPO\n",
    "def clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      discount=0.995,\n",
    "                      epsilon=0.1, beta=0.01):\n",
    "\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "    \n",
    "    # ratio for clipping\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "    # clipped function\n",
    "    clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    \n",
    "    # this returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T\n",
    "    # averaged over time-step and number of trajectories\n",
    "    # this is desirable because we have normalized our rewards\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   4% |#                                          | ETA:  3:03:06\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, \tscore: -12.88 \tAverage Score: -13.24\n",
      "[-10. -15.  -9. -11. -15. -13. -15. -15.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   8% |###                                        | ETA:  2:55:11\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 200, \tscore: -13.62 \tAverage Score: -13.08\n",
      "[-12. -13. -11. -15. -15. -14. -15. -14.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  13% |#####                                      | ETA:  2:47:08\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 300, \tscore: -12.38 \tAverage Score: -13.16\n",
      "[-14. -13. -12. -15. -14. -14.  -9.  -8.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  17% |#######                                    | ETA:  2:37:42\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 400, \tscore: -12.75 \tAverage Score: -12.65\n",
      "[-16. -14.  -8. -14. -13.  -8. -15. -14.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  21% |#########                                  | ETA:  2:29:35\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 500, \tscore: -10.88 \tAverage Score: -12.09\n",
      "[-11.  -8. -14. -12.  -8. -10. -13. -11.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  26% |###########                                | ETA:  2:21:22\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 600, \tscore: -10.75 \tAverage Score: -11.24\n",
      "[-13.  -9. -14.  -7. -13. -13. -11.  -6.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  30% |#############                              | ETA:  2:13:02\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 700, \tscore: -8.75 \tAverage Score: -10.12\n",
      "[ -9.  -7. -11. -10. -12.  -8.  -8.  -5.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  34% |##############                             | ETA:  2:04:23\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 800, \tscore: -7.50 \tAverage Score: -8.95\n",
      "[ -8. -12.  -7.  -2.  -6.  -7.  -9.  -9.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  39% |################                           | ETA:  1:56:12\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 900, \tscore: -5.75 \tAverage Score: -6.64\n",
      "[-8. -7. -7. -3. -6. -4. -5. -6.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  43% |##################                         | ETA:  1:46:50\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000, \tscore: -2.88 \tAverage Score: -3.61\n",
      "[ 0. -8. -2. -1. -6.  0.  0. -6.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  47% |####################                       | ETA:  1:37:50\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1100, \tscore: -1.25 \tAverage Score: -1.45\n",
      "[ 0.  0. -1.  0. -4. -3. -2.  0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  52% |######################                     | ETA:  1:29:37\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1200, \tscore: -0.38 \tAverage Score: -0.75\n",
      "[ 0.  1.  0.  0. -5.  0.  0.  1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  56% |########################                   | ETA:  1:21:37\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1300, \tscore: 0.00 \tAverage Score: -0.38\n",
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  60% |##########################                 | ETA:  1:13:35\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1400, \tscore: -0.12 \tAverage Score: -0.28\n",
      "[ 1.  0. -2.  0.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  65% |############################               | ETA:  1:05:31\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1500, \tscore: -0.75 \tAverage Score: -0.32\n",
      "[-1.  0.  1.  0. -1.  1. -4. -2.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  69% |#############################              | ETA:  0:57:32\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1600, \tscore: -0.75 \tAverage Score: -0.41\n",
      "[ 0.  0. -1.  1. -5. -1. -1.  1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  73% |###############################            | ETA:  0:49:21\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1700, \tscore: 1.12 \tAverage Score: 0.17\n",
      "[2. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  78% |#################################          | ETA:  0:41:09\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1800, \tscore: 0.88 \tAverage Score: 0.61\n",
      "[ 1.  1.  1.  1. -2.  1.  2.  2.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  82% |###################################        | ETA:  0:32:56\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1900, \tscore: 0.50 \tAverage Score: 0.74\n",
      "[ 1.  1.  1.  1. -3.  1.  1.  1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  86% |#####################################      | ETA:  0:24:43\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2000, \tscore: 0.62 \tAverage Score: 0.85\n",
      "[ 1.  1. -3.  2.  1.  1.  1.  1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  91% |#######################################    | ETA:  0:16:29\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2100, \tscore: 1.00 \tAverage Score: 0.93\n",
      "[1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  95% |#########################################  | ETA:  0:08:15\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2200, \tscore: 1.00 \tAverage Score: 0.95\n",
      "[1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop: 100% |###########################################| Time: 3:09:51\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2300, \tscore: 1.00 \tAverage Score: 0.97\n",
      "[1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "scores_deque = deque(maxlen=100)\n",
    "\n",
    "# progress bar\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "for e in range(episode):\n",
    "    \n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = collect_trajectories(envs, policy, tmax=tmax)\n",
    "    \n",
    "    # discount the rewards\n",
    "#     rewards = discounted_future_rewards(rewards, ratio=0.999)\n",
    "    \n",
    "    # get total rewards\n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "    scores_deque.append(total_rewards)\n",
    "    \n",
    "    # this is the SOLUTION!\n",
    "    # use your own surrogate function\n",
    "    L = -surrogate(policy, old_probs, states, actions, rewards, beta=beta) # minus because default in pytorch in gradient descent, we wanted gradient ascent here\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    L.backward()\n",
    "    optimizer.step()\n",
    "    del L\n",
    "        \n",
    "    # the regulation term also reduces, this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 100 iterations\n",
    "    if (e+1)%100 ==0 :\n",
    "        print(\"Episode: {}, \\tscore: {:.2f} \\tAverage Score: {:.2f}\" \\\n",
    "               .format(e+1, np.mean(total_rewards), np.mean(scores_deque)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# play game after training!\n",
    "play(env, model, time=2000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7facf78b95f8>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot mean rewards\n",
    "plt.plot(mean_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save agent\n",
    "# torch.save(model.state_dict(), 'surrogate_00003.policy')\n",
    "# torch.save(model.state_dict(), 'surrogate_clipped_00002.policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Notes\n",
    "\n",
    "- The performance for the REINFORCE version may be poor. You can try training with a smaller tmax=100 and more number of episodes=2000 to see concrete results.\n",
    "- Try normalizing your future rewards over all the parallel agents, it can speed up training\n",
    "- Simpler networks might perform better than more complicated ones! The original input contains 80x80x2=12800 numbers, you might want to ensure that this number steadily decreases at each layer of the neural net.\n",
    "- Training performance may be significantly worse on local machines. I had worse performance training on my own windows desktop with a 4-core CPU and a GPU. This may be due to the slightly different ways the emulator is rendered. So please run the code on the workspace first before moving locally\n",
    "- It may be beneficial to train multiple epochs, say first using a small tmax=200 with 500 episodes, and then train again with tmax = 400 with 500 episodes, and then finally with a even larger tmax.\n",
    "- Remember to save your policy after training!\n",
    "- for a challenge, try the 'Pong-v4' environment, this includes random frameskips and takes longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
